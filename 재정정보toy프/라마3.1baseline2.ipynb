{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5ef8d-298a-4171-8015-e54f04cd07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63af69d7-da13-4289-b20b-3a6dbcf2fa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 21:53:43.622715: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-17 21:53:43.622763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-17 21:53:43.623744: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-17 21:53:43.628790: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-17 21:53:44.398904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.677 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B\"  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbf17b3-5261-443a-8533-8d8b60812838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 101,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4aa72a4-d55d-462b-bd58-ea0e65a9daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForCausalLM\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Langchain ê´€ë ¨\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f78d56-5bbc-4dbe-a963-5a0ab4f4ecfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|                                                                                          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "Processing PDFs:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                            | 1/16 [00:05<01:15,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2024 ë‚˜ë¼ì‚´ë¦¼ ì˜ˆì‚°ê°œìš”...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                       | 2/16 [00:11<01:20,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ì¬ì •í†µê³„í•´ì„¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                  | 3/16 [00:16<01:10,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing êµ­í† êµí†µë¶€_ì „ì„¸ì„ëŒ€(ìœµì)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                             | 4/16 [00:20<00:57,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ê³ ìš©ë…¸ë™ë¶€_ì²­ë…„ì¼ìë¦¬ì°½ì¶œì§€ì›...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                        | 5/16 [00:23<00:45,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ê³ ìš©ë…¸ë™ë¶€_ë‚´ì¼ë°°ì›€ì¹´ë“œ(ì¼ë°˜)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                   | 6/16 [00:26<00:38,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ë³´ê±´ë³µì§€ë¶€_ë…¸ì¸ì¼ìë¦¬ ë° ì‚¬íšŒí™œë™ì§€ì›...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                              | 7/16 [00:29<00:32,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€_ì°½ì—…ì‚¬ì—…í™”ì§€ì›...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         | 8/16 [00:32<00:28,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ë³´ê±´ë³µì§€ë¶€_ìƒê³„ê¸‰ì—¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 9/16 [00:36<00:23,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing êµ­í† êµí†µë¶€_ì†Œê·œëª¨ì£¼íƒì •ë¹„ì‚¬ì—…...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 10/16 [00:39<00:19,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing êµ­í† êµí†µë¶€_ë¯¼ê°„ì„ëŒ€(ìœµì)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 11/16 [00:42<00:15,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ê³ ìš©ë…¸ë™ë¶€_ì¡°ê¸°ì¬ì·¨ì—…ìˆ˜ë‹¹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 12/16 [00:44<00:12,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2024ë…„ë„ ì„±ê³¼ê³„íšì„œ(ì´ê´„í¸)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 13/16 [00:51<00:12,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ã€ŒFIS ì´ìŠˆ & í¬ì»¤ìŠ¤ã€ 23-3í˜¸ ã€Šì¡°ì„¸ì§€ì¶œ ì—°ê³„ê´€ë¦¬ã€‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 14/16 [00:54<00:07,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ã€ŒFIS ì´ìŠˆ & í¬ì»¤ìŠ¤ã€ 22-3í˜¸ ã€Šì¬ì •ìœµìì‚¬ì—…ã€‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 15/16 [00:57<00:03,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ì›”ê°„ ë‚˜ë¼ì¬ì • 2023ë…„ 12ì›”í˜¸...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:01<00:00,  3.87s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00d5bbb7ce14561bd4d52846c97d62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SAMPLE_ID': 'TRAIN_000', 'Source': '1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ', 'Source_path': './train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf', 'Question': '2024ë…„ ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?', 'Answer': '2024ë…„ ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜Â·íŠ¹ë³„íšŒê³„)ê³¼ ê¸°ê¸ˆìœ¼ë¡œ êµ¬ë¶„ë˜ë©°, 2024ë…„ ê¸°ì¤€ìœ¼ë¡œ ì¼ë°˜íšŒê³„ 1ê°œ, íŠ¹ë³„íšŒê³„ 21ê°œ, ê¸°ê¸ˆ 68ê°œë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n2024ë…„ ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?\\n\\n### Input:\\n2. ì¬ì •ìˆ˜ì…\\n3. ì¬ì •ì§€ì¶œ\\n4. ì§€ë°©ì¬ì • ì¡°ì •\\n5. ì´ì‚¬ì—…ë¹„ ê´€ë¦¬ëŒ€ìƒì‚¬ì—…\\n6. ê³„ì†ë¹„ ëŒ€ìƒì‚¬ì—…\\nì£¼ìš” ì¬ì •í†µê³„\\nâ—\\nâ… .\\n2\\n01 ì¬ì •ì²´ê³„\\nâ–¸ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜ï½¥íŠ¹ë³„íšŒê³„)ê³¼ ê¸°ê¸ˆìœ¼ë¡œ êµ¬ë¶„ë˜ë©°, 2024ë…„ ê¸°ì¤€ìœ¼ë¡œ ì¼ë°˜íšŒê³„ 1ê°œ, íŠ¹ë³„íšŒê³„ \\n21ê°œ, ê¸°ê¸ˆ 68ê°œë¡œ êµ¬ì„±\\nâˆ™2024ë…„ ì˜ˆì‚° ì§€ì¶œì€ ì¼ë°˜íšŒê³„ 356.5ì¡°ì›, 21ê°œ íŠ¹ë³„íšŒê³„ 81.7ì¡°ì›ìœ¼ë¡œ êµ¬ì„±\\nâˆ™2024ë…„ ê¸°ê¸ˆ ì§€ì¶œì€ 49ê°œ ì‚¬ì—…ì„±ê¸°ê¸ˆ 81.2ì¡°ì›, 6ê°œ ì‚¬íšŒë³´í—˜ì„±ê¸°ê¸ˆ 107.1ì¡°ì›, 5ê°œ ê³„ì •ì„±ê¸°ê¸ˆ \\n30.1ì¡°ì›ìœ¼ë¡œ êµ¬ì„±\\n[ê·¸ë¦¼ 1-1] ì¬ì •ì§€ì¶œ êµ¬ì¡°(2024ë…„ ì˜ˆì‚° ì´ì§€ì¶œ ê¸°ì¤€)\\nì£¼: ê´„í˜¸ ì•ˆì€ ì´ê³„ ê¸°ì¤€ ì˜ˆì‚°ì•¡ì„ ì˜ë¯¸\\nìë£Œ: ë””ì§€í„¸ì˜ˆì‚°íšŒê³„ì‹œìŠ¤í…œ\\n2024 ì£¼ìš” ì¬ì •í†µê³„ | 2024 Fiscal Statistics\\nâ… . ì£¼ìš”ì¬ì •í†µê³„\\nâ…¡. êµ­ì œí†µê³„\\në¶€ë¡\\nâ…¢. ë¶„ì•¼ë³„ ì¬ì •ì§€ì¶œ\\nâ… . ì£¼ìš”ì¬ì •í†µê³„\\n3\\nâ–¸ì˜ˆì‚°ì€ ï½¢êµ­ê°€ì¬ì •ë²•ï½£ì— ê·¼ê±°í•´ ì •ë¶€ê°€ í¸ì„±í•˜ê³  êµ­íšŒê°€ ì‹¬ì˜ï½¥ì˜ê²°ë¡œ í™•ì •í•œ ì¬ì •ì§€ì¶œê³„íšì„ ì˜ë¯¸í•˜ë©°, \\nì¼ë°˜íšŒê³„ì™€ íŠ¹ë³„íšŒê³„ë¡œ êµ¬ë¶„\\n\\n### Response:\\n2024ë…„ ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜Â·íŠ¹ë³„íšŒê³„)ê³¼ ê¸°ê¸ˆìœ¼ë¡œ êµ¬ë¶„ë˜ë©°, 2024ë…„ ê¸°ì¤€ìœ¼ë¡œ ì¼ë°˜íšŒê³„ 1ê°œ, íŠ¹ë³„íšŒê³„ 21ê°œ, ê¸°ê¸ˆ 68ê°œë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.<|end_of_text|>'}\n"
     ]
    }
   ],
   "source": [
    "# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°\n",
    "def process_pdf(file_path, chunk_size=512, chunk_overlap=32):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = ''\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunk_temp = splitter.split_text(text)\n",
    "    chunks = [Document(page_content=t) for t in chunk_temp]\n",
    "    return chunks\n",
    "\n",
    "# FAISS DB ìƒì„± í•¨ìˆ˜\n",
    "def create_vector_db(chunks, model_path=\"intfloat/multilingual-e5-base\"):\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return db\n",
    "\n",
    "# ê²½ë¡œ ìœ ë‹ˆì½”ë“œ ì •ê·œí™” í•¨ìˆ˜\n",
    "def normalize_path(path):\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "\n",
    "# PDF ë°ì´í„°í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ì—¬ DB ë° retriever ìƒì„±\n",
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        chunks = process_pdf(full_path)\n",
    "        db = create_vector_db(chunks)\n",
    "        \n",
    "        retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3, 'fetch_k': 8})\n",
    "        \n",
    "        pdf_databases[pdf_title] = {\n",
    "            'db': db,\n",
    "            'retriever': retriever\n",
    "        }\n",
    "    return pdf_databases\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í…ìŠ¤íŠ¸ ì¡°ê° ì°¾ê¸°\n",
    "def find_relevant_chunk(question, pdf_databases):\n",
    "    \"\"\"ì§ˆë¬¸ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ chunk ì°¾ê¸°\"\"\"\n",
    "    relevant_chunk = None\n",
    "    for pdf_title, data in pdf_databases.items():\n",
    "        retriever = data['retriever']\n",
    "        relevant_docs = retriever.get_relevant_documents(question)\n",
    "        if relevant_docs:\n",
    "            relevant_chunk = relevant_docs[0].page_content\n",
    "            break  # ê°€ì¥ ìœ ì‚¬í•œ chunkë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ë°˜ë³µ ì¢…ë£Œ\n",
    "    return relevant_chunk if relevant_chunk else \"\"\n",
    "\n",
    "# Alpaca í¬ë§· ì •ì˜\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# ë°ì´í„° í¬ë§·íŒ… í•¨ìˆ˜ ì •ì˜\n",
    "def formatting_prompts_func(examples, pdf_databases):\n",
    "    questions = examples[\"Question\"]\n",
    "    answers = examples[\"Answer\"]\n",
    "    texts = []\n",
    "\n",
    "    for question, answer in zip(questions, answers):\n",
    "        # ì§ˆë¬¸ì— ê°€ì¥ ìœ ì‚¬í•œ chunkë¥¼ ì°¾ìŒ\n",
    "        relevant_chunk = find_relevant_chunk(question, pdf_databases)\n",
    "        if relevant_chunk:\n",
    "            # Alpaca í¬ë§·ìœ¼ë¡œ ë³€í™˜\n",
    "            text = alpaca_prompt.format(question, relevant_chunk, answer) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        else:\n",
    "            # ìœ ì‚¬í•œ chunkê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ í…ìŠ¤íŠ¸ í˜•ì‹ (ì˜µì…˜)\n",
    "            text = alpaca_prompt.format(question, \"\", answer) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# CSV ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "csv_path = \"/home/jovyan/temp/open/train.csv\"  # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "dataset = load_dataset('csv', data_files=csv_path, split='train')\n",
    "\n",
    "# PDF ë°ì´í„° ì²˜ë¦¬ ë° DB, retriever ìƒì„±\n",
    "base_directory = './'\n",
    "csv_df = pd.read_csv(csv_path)\n",
    "pdf_databases = process_pdfs_from_dataframe(csv_df, base_directory)\n",
    "\n",
    "# í¬ë§·íŒ… í•¨ìˆ˜ ì ìš©\n",
    "formatted_dataset = dataset.map(lambda x: formatting_prompts_func(x, pdf_databases), batched=True)\n",
    "\n",
    "# í¬ë§·íŒ…ëœ ë°ì´í„° í™•ì¸\n",
    "print(formatted_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "502bb4e4-eabb-4d12-b02f-4ca8a2e749c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SAMPLE_ID': 'TRAIN_001', 'Source': '1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ', 'Source_path': './train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf', 'Question': '2024ë…„ ì¤‘ì•™ì •ë¶€ì˜ ì˜ˆì‚° ì§€ì¶œì€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?', 'Answer': '2024ë…„ ì¤‘ì•™ì •ë¶€ì˜ ì˜ˆì‚° ì§€ì¶œì€ ì¼ë°˜íšŒê³„ 356.5ì¡°ì›, 21ê°œ íŠ¹ë³„íšŒê³„ 81.7ì¡°ì›ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n2024ë…„ ì¤‘ì•™ì •ë¶€ì˜ ì˜ˆì‚° ì§€ì¶œì€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?\\n\\n### Input:\\n2. ì¬ì •ìˆ˜ì…\\n3. ì¬ì •ì§€ì¶œ\\n4. ì§€ë°©ì¬ì • ì¡°ì •\\n5. ì´ì‚¬ì—…ë¹„ ê´€ë¦¬ëŒ€ìƒì‚¬ì—…\\n6. ê³„ì†ë¹„ ëŒ€ìƒì‚¬ì—…\\nì£¼ìš” ì¬ì •í†µê³„\\nâ—\\nâ… .\\n2\\n01 ì¬ì •ì²´ê³„\\nâ–¸ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜ï½¥íŠ¹ë³„íšŒê³„)ê³¼ ê¸°ê¸ˆìœ¼ë¡œ êµ¬ë¶„ë˜ë©°, 2024ë…„ ê¸°ì¤€ìœ¼ë¡œ ì¼ë°˜íšŒê³„ 1ê°œ, íŠ¹ë³„íšŒê³„ \\n21ê°œ, ê¸°ê¸ˆ 68ê°œë¡œ êµ¬ì„±\\nâˆ™2024ë…„ ì˜ˆì‚° ì§€ì¶œì€ ì¼ë°˜íšŒê³„ 356.5ì¡°ì›, 21ê°œ íŠ¹ë³„íšŒê³„ 81.7ì¡°ì›ìœ¼ë¡œ êµ¬ì„±\\nâˆ™2024ë…„ ê¸°ê¸ˆ ì§€ì¶œì€ 49ê°œ ì‚¬ì—…ì„±ê¸°ê¸ˆ 81.2ì¡°ì›, 6ê°œ ì‚¬íšŒë³´í—˜ì„±ê¸°ê¸ˆ 107.1ì¡°ì›, 5ê°œ ê³„ì •ì„±ê¸°ê¸ˆ \\n30.1ì¡°ì›ìœ¼ë¡œ êµ¬ì„±\\n[ê·¸ë¦¼ 1-1] ì¬ì •ì§€ì¶œ êµ¬ì¡°(2024ë…„ ì˜ˆì‚° ì´ì§€ì¶œ ê¸°ì¤€)\\nì£¼: ê´„í˜¸ ì•ˆì€ ì´ê³„ ê¸°ì¤€ ì˜ˆì‚°ì•¡ì„ ì˜ë¯¸\\nìë£Œ: ë””ì§€í„¸ì˜ˆì‚°íšŒê³„ì‹œìŠ¤í…œ\\n2024 ì£¼ìš” ì¬ì •í†µê³„ | 2024 Fiscal Statistics\\nâ… . ì£¼ìš”ì¬ì •í†µê³„\\nâ…¡. êµ­ì œí†µê³„\\në¶€ë¡\\nâ…¢. ë¶„ì•¼ë³„ ì¬ì •ì§€ì¶œ\\nâ… . ì£¼ìš”ì¬ì •í†µê³„\\n3\\nâ–¸ì˜ˆì‚°ì€ ï½¢êµ­ê°€ì¬ì •ë²•ï½£ì— ê·¼ê±°í•´ ì •ë¶€ê°€ í¸ì„±í•˜ê³  êµ­íšŒê°€ ì‹¬ì˜ï½¥ì˜ê²°ë¡œ í™•ì •í•œ ì¬ì •ì§€ì¶œê³„íšì„ ì˜ë¯¸í•˜ë©°, \\nì¼ë°˜íšŒê³„ì™€ íŠ¹ë³„íšŒê³„ë¡œ êµ¬ë¶„\\n\\n### Response:\\n2024ë…„ ì¤‘ì•™ì •ë¶€ì˜ ì˜ˆì‚° ì§€ì¶œì€ ì¼ë°˜íšŒê³„ 356.5ì¡°ì›, 21ê°œ íŠ¹ë³„íšŒê³„ 81.7ì¡°ì›ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.<|end_of_text|>'}\n"
     ]
    }
   ],
   "source": [
    "# í¬ë§·íŒ…ëœ ë°ì´í„° í™•ì¸\n",
    "print(formatted_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a621b34-6e55-4c3f-ae79-8ffa36375647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848e921f2fa4458c9d21fc31c54aa790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=60,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,  # ë˜ëŠ” bf16=True, ë˜ëŠ” ë‘˜ ë‹¤ Falseë¡œ ì„¤ì •\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=101,\n",
    "    output_dir=\"outputs\",\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deb9d412-36d9-47a2-9054-0608ba996d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 496 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 4 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.882900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.631800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.926300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.656500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.568100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.611700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.341500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.504900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.431600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.457300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.344600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.331100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.367800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.819500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.372200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.182300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.935900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.225600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í›ˆë ¨ ì‹¤í–‰\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e95018d-e888-4c0d-984d-89da23e7f487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nê¸°ê¸ˆê³¼ ì˜ˆì‚°ì˜ ë‹¤ë¥¸ì ì€?\\n\\n### Input:\\n\\n\\n### Response:\\nê¸°ê¸ˆì€ ì¬ì •ìˆ˜ì§€ì™€ ë¬´ê´€í•˜ë©°, ì¬ì •ìˆ˜ì§€ì— ì§ì ‘ì ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤.<|end_of_text|>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"ê¸°ê¸ˆê³¼ ì˜ˆì‚°ì˜ ë‹¤ë¥¸ì ì€?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5419de-167f-4f88-bf1f-12f411804c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2024ë…„ ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜Â·íŠ¹ë³„íšŒê³„)ê³¼ ê¸°ê¸ˆìœ¼ë¡œ êµ¬ë¶„ë˜ë©°, 2024ë…„ ê¸°ì¤€ìœ¼ë¡œ ì¼ë°˜íšŒê³„ 1ê°œ, íŠ¹ë³„íšŒê³„ 21ê°œ, ê¸°ê¸ˆ 68ê°œë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb24df-6c4b-4e51-9de1-aae26d436c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(\"llama3_model_ê±°ì˜ìµœì¢…\") # Local saving\n",
    "#tokenizer.save_pretrained(\"llama3_model_ê±°ì˜ìµœì¢…\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556e3a9-cd6d-41af-bae8-ccea01d16043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
